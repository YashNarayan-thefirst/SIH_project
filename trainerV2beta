import json
import nltk
import tensorflow as tf
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import numpy as np

# Define the maximum data size limit in bytes (10 MB)
max_data_size_limit = 10 * 1024 * 1024  # 10 MB in bytes

# Initialize variables to keep track of data size
total_data_size = 0

# Initialize lists for storing input and output data
input_data = []
output_data = []
y_train = []  # Initialize y_train list

# Keep track of seen labels using a dictionary
label_mapping = {}
current_label_id = 0

# Initialize NLTK stopwords
nltk.download('stopwords')
stop_words = set(nltk.corpus.stopwords.words('english'))

def preprocess_text(text):
    # Tokenize, remove stopwords, and lemmatize the text
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word.lower() not in stop_words]
    lemmatizer = nltk.stem.WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

def process_data(data):
    # Vectorize the data using TF-IDF
    vectorizer = TfidfVectorizer(
        max_features=5000,
        tokenizer=word_tokenize,
        ngram_range=(1, 1),
    )
    tfidf_vectors = vectorizer.fit_transform(data)
    return tfidf_vectors.toarray()  # Convert sparse matrix to dense NumPy array

def extract_labels_from_output(output_text):
    labels = set()  # Use a set to ensure unique labels
    
    # Split the text into sentences based on line breaks
    sentences = output_text.split('\n')
    
    # Iterate through the sentences and extract labels
    for sentence in sentences:
        # Use a regular expression to match lines starting with a bullet point
        match = re.match(r'â€¢\s*(.*)', sentence)
        if match:
            label = match.group(1).strip()  # Extract the label text
            labels.add(label)  # Add the label to the set
    
    return list(labels)

# Create a neural network model
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(5000,)),  # Define the input shape (TF-IDF vector size)
    tf.keras.layers.Dense(128, activation='relu'),  # Add a dense layer with ReLU activation
    tf.keras.layers.Dropout(0.5),  # Add dropout for regularization
    tf.keras.layers.Dense(64, activation='relu'),  # Add another dense layer with ReLU activation
    tf.keras.layers.Dropout(0.5),  # Add dropout for regularization
    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer with sigmoid activation
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Read and process the dataset
with open('dataset.jsonl', 'r', encoding='utf-8') as jsonl_file:
    for line in jsonl_file:
        json_data = json.loads(line)
        input_text = json_data["input"]
        output_text = json_data["output"]
        y_train.append(input_text)  # Store input_text in y_train

        # Calculate the size of the current chunk in bytes
        chunk_size = len(input_text.encode('utf-8')) + len(output_text.encode('utf-8'))

        # Check if adding the current chunk exceeds the data size limit
        if total_data_size + chunk_size > max_data_size_limit:
            print("Data size limit exceeded. Preprocessing and training on current chunks...")

            # Preprocess the input and output data
            preprocessed_input = [preprocess_text(text) for text in input_data]
            preprocessed_output = [preprocess_text(text) for text in output_data]

            # Extract relevant labels from the preprocessed output text
            labels = extract_labels_from_output('\n'.join(preprocessed_output))

            # Map labels to numerical IDs and add new labels to the label_mapping dictionary
            for label in labels:
                if label not in label_mapping:
                    label_mapping[label] = current_label_id
                    current_label_id += 1

            # Create a list of label IDs for the current data
            data_labels = np.array([label_mapping[label] for label in labels])

            # Combine input and output data into a single list
            data = [input_text + ' ' + output_text for input_text, output_text in zip(preprocessed_input, preprocessed_output)]

            # Vectorize the data
            tfidf_vectors = process_data(data)

            # Define the split ratio (e.g., 0.8 for 80% training, 0.2 for 20% testing)
            split_ratio = 0.8
            split_index = int(tfidf_vectors.shape[0] * split_ratio)

            # Split the data into training and testing sets
            X_train, X_test = tfidf_vectors[:split_index], tfidf_vectors[split_index:]
            y_train_data, y_test_data = data_labels[:split_index], data_labels[split_index:]

            # Check if there are labels for the current data
            if y_train_data.size > 0:
                # Train the neural network model on the current data
                model.fit(X_train, y_train_data, epochs=10, batch_size=32, validation_data=(X_test, y_test_data))

            # Clear the processed data lists
            input_data.clear()
            output_data.clear()
            total_data_size = 0

            print("Cleared processed data lists.")
            
            # Exit the loop to skip processing remaining data
            break  # Add this line

        # Add the current data to the lists
        input_data.append(input_text)
        output_data.append(output_text)
        total_data_size += chunk_size

# Train the model on any remaining data if the limit is not exceeded
if input_data:
    print("Training on remaining data...")
    
    # Preprocess the input and output data for remaining data
    preprocessed_input = [preprocess_text(text) for text in input_data]
    preprocessed_output = [preprocess_text(text) for text in output_data]
    
    # Extract relevant labels from the preprocessed output text
    remaining_labels = extract_labels_from_output('\n'.join(preprocessed_output))
    
    # Map labels to numerical IDs and add new labels to the label_mapping dictionary
    for label in remaining_labels:
        if label not in label_mapping:
            label_mapping[label] = current_label_id
            current_label_id += 1

    # Create a list of label IDs for the remaining data
    data_labels = np.array([label_mapping[label] for label in remaining_labels])

    # Combine input and output data into a single list for remaining data
    data = [input_text + ' ' + output_text for input_text, output_text in zip(preprocessed_input, preprocessed_output)]

    # Vectorize the data for remaining data
    tfidf_vectors = process_data(data)

    # Check if there are labels for the remaining data
    if data_labels.size > 0:
        # Train the neural network model on the remaining data
        model.fit(tfidf_vectors, data_labels, epochs=10, batch_size=32)

    # Save the trained model
    model.save("model.h5")
    print("Model saved as model.h5")
